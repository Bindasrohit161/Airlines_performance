Model building
=======================

This example file will walk you through the steps involved to build an
ML model using historic data and predict on new incoming data.

The example provides historic Airline delay data with respect to Airports.
A classification model will be built based on their class of ailines routs
and their on time and delay patterns.

Building an ML model and predicting on RapidCanvas involves the
following steps:


-  Import functions
-  Authenticate your client
-  Create a custom environment
-  Create a new project

-  Split data for training and validation
-  Create a build pipeline

   -  Add Input Datasets
   -  Transform your raw data

   -  Create recipe for feature engineering
   -  Create recipe to Clustering data
   -  Build ML model

-  Create a Predict pipeline
   -  Create recipe for feature engineering
   -  Predict new data


Import function
---------------

.. code:: ipython3

    from utils.rc.client.requests import Requests
    from utils.rc.client.auth import AuthClient

    from utils.rc.dtos.project import Project
    from utils.rc.dtos.dataset import Dataset
    from utils.rc.dtos.recipe import Recipe
    from utils.rc.dtos.transform import Transform
    from utils.rc.dtos.template import Template, TemplateTransform, TemplateInput
    from utils.rc.dtos.template_v2 import TemplateV2, TemplateTransformV2
    from utils.rc.dtos.env import Env
    from utils.rc.dtos.env import EnvType
    import json
    import pandas as pd
    import logging
    logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.INFO)

Authenticate your client
------------------------

.. code:: ipython3

    
    AuthClient.setToken() #you can find your token in RapidCanvas UI under tools/token

Create a custom environment
---------------------------

Here are the available custom environments and their usage gudelines

| SMALL: 1 Core, 2GB Memmory
| MEDIUM: 2 Cores, 4GB Memmory
| LARGE: 4 Cores, 8GB Memmory
| CPU_LARGE: 8 Cores, 16GB Memmory
| MAX_LARGE: 12 Cores, 32GB Memmory
| EXTRA_MAX_LARGE: 12 Cores, 48GB Memmory

.. code:: ipython3

    ## Environment Creation
    Airline_performance = Env.createEnv(
        name="Airline_Performance",
        description="env for my Airline_performance Prediction project",
        envType=EnvType.CPU_LARGE, #pick one of the pre-defined configs
    #     requirements="matplotlib seaborn scipy  networkx sklearn xgboost shap"
        requirements="networkx shapely xgboost shap-hypetune" 
    )

Create a Project
----------------

Create a new project under your tenant

.. code:: ipython3

    ### Create project on platform
    project = Project.create(
        name='Airline_performance',
        description='AirLine Airline_performance prediction',
        createEmpty=True,
        envId=Airline_performance.id,

    )
    project.id
**This has now created a new project named â€œBuild and Predictâ€ under
your tenant. You can check the same on the RapidCanvas UI by logging in
here:** `RapidCanvas UI <https://staging.dev.rapidcanvas.net/>`__

Split data for training and validation
----------------------------------------

As part of built and predict pipeline, we are splitting data here
for tarin and validation data

.. code:: ipython3
    Train_val_split=project.addRecipe([train], name='Train_val_Split')

    Train_val_split_template = TemplateV2(
        name="Train_val_split", description="Train_val_split", project_id=project.id,
        source="CUSTOM", status="ACTIVE", tags=["UI", "Scalar"]
    )
    Train_val_split_template_transform = TemplateTransformV2(
        type = "python", params=dict(notebookName="Train_val_split.ipynb"))

    Train_val_split_template.base_transforms = [Train_val_split_template_transform]
    Train_val_split_template.publish("transforms/Train_val_split.ipynb")

    Train_val_split_template_transform = Transform()
    Train_val_split_template_transform.templateId = Train_val_split_template.id
    Train_val_split_template_transform.name='Train_val'
    Train_val_split_template_transform.variables = {
        'inputDataset': 'otp_time_series_web',

        'Airline': 'Airline',
        'Sectors_Scheduled': 'Sectors_Scheduled',

        'Sectors_Flown': 'Sectors_Flown',



        'Cancellations': 'Cancellations',

        'Departures_Delayed': 'Departures_Delayed',
        'Departures_On_Time' : 'Departures_On_Time',


        'Arrivals_Delayed': 'Arrivals_Delayed',
        'Departing_Port': 'Departing_Port',
        'Arriving_Port': 'Arriving_Port',

        'Month_Num' : 'Month_Num',
        'Year'     : 'Year',


        'outputDataset': 'airline',
        'outputDataset2': 'airline_val'
    }

    Train_val_split.add_transform(Train_val_split_template_transform)
    Train_val_split.run()


Create a build pipeline
-----------------------

As part of the section, we will follow all the relevant steps to build
an ML model using historic data.

Add Input Datasets - Build pipeline
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As part of the build pipeline, we are adding 1 data set to the project,
Delay data contains Airline schedules, sectore of departure and arrival,
Airline cacellation count every month, Number airline departure delayed,
number of airline dparture on time, number of airline arrived on time,
number of flight arrived late, number of airline depearted from sector, 
number of airline arrived on port,month of data, year of data.


.. code:: ipython3

    train = project.addDataset(
        dataset_name="otp_time_series_web",
         dataset_description="All_airline",
         data_source_id=dataSource.id,
         data_source_options={GcpConfig.FILE_PATH: 'Airline_performance/otp_time_series_web.csv'} #saving output with csv input in recipe not working properly
    )


Transform your raw data
-----------------------

Create recipe and template for feature engineering
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

this recipe use the  data and add some features in combined one data format 
w.r.t important features which we exlplored from Shap and stastical understanding.

Few features examples can see below-

Adding New Feature :: Total_scheduled
Adding New Feature :: Total_departed
Adding New Feature :: Cancelled_perc
Adding New Feature :: departure_delay_perc
Adding New Feature :: arrival_delay_perc
Adding New Feature :: Dep_Arr_delay
Adding New Feature :: Btwness
Adding New Feature :: closeness
Adding New Feature :: Deg_centrality


Note- You can see the Feature engineering code from transform folder in Feature_Enginerring.ipynb file.


.. code:: ipython3

    feature_engineering = project.addRecipe([train], name='Feature_Engineering',condition=build_mode)
    feature_engineering_template = TemplateV2(
        name="feature_engineering", description="1st feature_engineering", project_id=project.id,
        source="CUSTOM", status="ACTIVE", tags=["UI", "Scalar"]
    )
    feature_engineering_transform = TemplateTransformV2(
        type = "python", params=dict(notebookName="Cleaning&Feature_Engineering_Raw_data.ipynb"))

    feature_engineering_template.base_transforms = [feature_engineering_transform]
    feature_engineering_template.publish("transforms/Cleaning&Feature_Engineering_Raw_data.ipynb")

    feature_engineering_transform = Transform()
    feature_engineering_transform.templateId = feature_engineering_template.id
    feature_engineering_transform.name='features'
    feature_engineering_transform.variables = {
        'inputDataset': 'airline',

        'Airline': 'Airline',
        'Sectors_Scheduled': 'Sectors_Scheduled',
        'Total_scheduled': 'Total_scheduled',
        'Sectors_Flown': 'Sectors_Flown',
        'Total_departed': 'Total_departed',

         'Cancelled_perc': 'Cancelled_perc',
        'Cancellations': 'Cancellations',
        'departure_delay_perc': 'departure_delay_perc',
        'Departures_Delayed': 'Departures_Delayed',
        'Departures_On_Time' : 'Departures_On_Time',
        'arrival_delay_perc': 'arrival_delay_perc',

        'Dep_Arr_delay': 'Dep_Arr_delay',
        'Arrivals_Delayed': 'Arrivals_Delayed',
        'Departing_Port': 'Departing_Port',
        'Arriving_Port': 'Arriving_Port',

        'Month_Num' : 'Month_Num',
        'Year'     : 'Year',
         'Btwness': 'Btwness',
        'closeness': 'closeness',
        'Deg_centrality': 'Deg_centrality',
    #     'useless' : 'useless',
        'Airline_rout_chart': 'Airline_route_graph',


        'outputDataset': 'airline_grp'
    }

    feature_engineering.add_transform(feature_engineering_transform)
    feature_engineering.run()



Output dataset and review sample
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    airline_grp = feature_engineering.getChildrenDatasets()['airline_grp']
    airline_grp.getData(5)




Create recipe and template to Clustering data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

This recipe use for clustering on featured engineered data w.r.t to pattern of 
airlines from departure to arrival port.

Few Data aggregation examples can see below-

   -  Claim Duration for Potentially Fraud & Non-Fraud Providers.
   -  Distribution of Fraud & Non-fraud claims.
   -  Claim Duration of both the genders for Potentially Fraud & Non-Fraud Providers.
   -  Claim Duration patient life status for Potentially Fraud & Non-Fraud Providers.
   -  Claim Re-Imb Amount for Potentially Fraud & Non-Fraud Providers.
   -  Annual IP Re-Imb Amount for Potentially Fraud & Non-Fraud Providers.
   -  Claim Re-Imb Amount of all human races for Potentially Fraud & Non-Fraud Providers.


	

Note- You can see the data clustering code from transform folder in clustering.ipynb file.


.. code:: ipython3

    clustering_template = TemplateV2(
        name="Clustering", description="Clustering for class labels", project_id=project.id,
        source="CUSTOM", status="ACTIVE", tags=["UI", "Scalar"]
    )
    clustering_template_transform = TemplateTransformV2(
        type = "python", params=dict(notebookName="Clustering.ipynb"))

    clustering_template.base_transforms = [clustering_template_transform]
    clustering_template.publish("transforms/Clustering.ipynb")

    clustering_template_transform = Transform()
    clustering_template_transform.templateId = clustering_template.id
    clustering_template_transform.name='clustering'
    clustering_template_transform.variables = {
        'inputDataset2': 'airline_grp',
            'Airline': 'Airline',
    #     'Sectors_Scheduled': 'Sectors_Scheduled',
        'Total_scheduled': 'Total_scheduled',
    #     'Sectors_Flown': 'Sectors_Flown',
        'Total_departed': 'Total_departed',

         'Cancelled_perc': 'Cancelled_perc',
        'Cancellations': 'Cancellations',
        'departure_delay_perc': 'departure_delay_perc',
        'Departures_Delayed': 'Departures_Delayed',
        'Departures_On_Time' : 'Departures_On_Time',
        'arrival_delay_perc': 'arrival_delay_perc',

        'Dep_Arr_delay': 'Dep_Arr_delay',
        'Arrivals_Delayed': 'Arrivals_Delayed',
    #     'Departing_Port': 'Departing_Port',
    #     'Arriving_Port': 'Arriving_Port',

    #     'Month_Num' : 'Month_Num',
    #     'Year'     : 'Year',
         'Btwness': 'Btwness',
        'closeness': 'closeness',
        'Deg_centrality': 'Deg_centrality',




        'label': 'label',


        'outputDataset2': 'All_airline_clust2'
    }

    clustering.add_transform(clustering_template_transform)
    clustering.run()



Output dataset and review sample
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. code:: ipython3

    clust_data =clustering.getChildrenDatasets()['All_airline_clust2']
    clust_data.getData(5)



Build ML model
--------------

Add the dataset
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

As part of the model building step, we first feature engineered and clustered the historic data
which will be input for our classification model.

.. code:: ipython3

    Train_test_split=project.addRecipe([clust_data], name='Classification',condition=build_mode)

Create a recipe to build a XGboos classification model with SHapley analysis
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this step we build the ML models. Please note that once the models
built, it is automatically stored in RapidCanvas repository and can be
retrieved for prediction in the later steps.

We use Sklearn, Shapley and XGboost for classifcation model.

Note that this marks the end of the build pipeline

.. code:: ipython3

    Train_test_split_template = TemplateV2(
        name="Classification", description="Classification", project_id=project.id,
        source="CUSTOM", status="ACTIVE", tags=["UI", "Scalar"]
    )
    Train_test_split_template_transform = TemplateTransformV2(
        type = "python", params=dict(notebookName="Train_Test_split_Model.ipynb"))

    Train_test_split_template.base_transforms = [Train_test_split_template_transform]
    Train_test_split_template.publish("transforms/Train_Test_split_Model.ipynb")


.. code:: ipython3

    Train_test_split_template_transform = Transform()
    Train_test_split_template_transform.templateId = Train_test_split_template.id
    Train_test_split_template_transform.name='Classification'
    Train_test_split_template_transform.variables = {
        'inputDataset3': 'All_airline_clust2',

    #     'Sectors_Scheduled': 'Sectors_Scheduled',
        'Total_scheduled': 'Total_scheduled',
    #     'Sectors_Flown': 'Sectors_Flown',
        'Total_departed': 'Total_departed',

         'Cancelled_perc': 'Cancelled_perc',
        'Cancellations': 'Cancellations',
        'departure_delay_perc': 'departure_delay_perc',
        # 'Departures_Delayed': 'Departures_Delayed',
        'Departures_On_Time' : 'Departures_On_Time',
        'arrival_delay_perc': 'arrival_delay_perc',

        'Dep_Arr_delay': 'Dep_Arr_delay',
        'Arrivals_Delayed': 'Arrivals_Delayed',
    #     'Departing_Port': 'Departing_Port',
    #     'Arriving_Port': 'Arriving_Port',

    #     'Month_Num' : 'Month_Num',
    #     'Year'     : 'Year',
         'Btwness': 'Btwness',
        'closeness': 'closeness',
        'Deg_centrality': 'Deg_centrality',




        'label': 'label',
        'model_to_save':'XGB_Shap_model',
        'outputDataset3': 'result'


        # 'outputDataset3': 'XTrain',
        # 'outputDataset4': 'XTest',
        # 'outputDataset5': 'YTrain',
        # 'outputDataset6': 'YTest',

    }

    Train_test_split.add_transform(Train_test_split_template_transform)

.. code:: ipython3

    Train_test_split.run()



Create a Predict pipeline
=========================

Feature engineering of validation data
--------------------------------------------
Here we are doing feture engineering on validation data which will be input for 
prediction pipeline.

Create a recipe and template to transform validation data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: ipython3

    feature_engineering_predict = project.addRecipe([val], name='feature_engineering_predict',condition=predict_mode)

.. code:: ipython3

    feature_engineering_predict_template = TemplateV2(
    name="feature_engineering_predict", description="feature_engineering prediction", project_id=project.id,
    source="CUSTOM", status="ACTIVE", tags=["UI", "Scalar"]
    )
    feature_engineering_predict_template_transform = TemplateTransformV2(
        type = "python", params=dict(notebookName="Cleaning&Feature_Engineering_Raw_data.ipynb"))

    feature_engineering_predict_template.base_transforms = [feature_engineering_predict_template_transform]
    feature_engineering_predict_template.publish("transforms/Cleaning&Feature_Engineering_Raw_data.ipynb")

.. code:: ipython3

    feature_engineering_predict_transform = Transform()
    feature_engineering_predict_transform.templateId = feature_engineering_predict_template.id
    feature_engineering_predict_transform.name='features'
    feature_engineering_predict_transform.variables = {
        'inputDataset': 'airline_val',

        'Airline': 'Airline',
        'Sectors_Scheduled': 'Sectors_Scheduled',
        'Total_scheduled': 'Total_scheduled',
        'Sectors_Flown': 'Sectors_Flown',
        'Total_departed': 'Total_departed',

         'Cancelled_perc': 'Cancelled_perc',
        'Cancellations': 'Cancellations',
        'departure_delay_perc': 'departure_delay_perc',
        'Departures_Delayed': 'Departures_Delayed',
        'Departures_On_Time' : 'Departures_On_Time',
        'arrival_delay_perc': 'arrival_delay_perc',

        'Dep_Arr_delay': 'Dep_Arr_delay',
        'Arrivals_Delayed': 'Arrivals_Delayed',
        'Departing_Port': 'Departing_Port',
        'Arriving_Port': 'Arriving_Port',

        'Month_Num' : 'Month_Num',
        'Year'     : 'Year',
         'Btwness': 'Btwness',
        'closeness': 'closeness',
        'Deg_centrality': 'Deg_centrality',
    #     'useless' : 'useless',

        'outputDataset': 'airline_grp_predict'
    }

    feature_engineering_predict.add_transform(feature_engineering_predict_transform)

.. code:: ipython3

    feature_engineering_predict.run()





Prediction on validatio data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can use feture engineered validation data as a input for saved trained model clasification.

Create a recipe and template to predict 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code:: ipython3

    prediction_recipe=project.addRecipe([Val_data], name='ML Prediction',condition=predict_mode)


.. code:: ipython3

    prediction_recipe_template = TemplateV2(
        name="Validation_predict", description="prediction", project_id=project.id,
        source="CUSTOM", status="ACTIVE", tags=["UI", "Scalar"]
    )
    prediction_recipe_transform = TemplateTransformV2(
        type = "python", params=dict(notebookName="Prediction.ipynb"))

    prediction_recipe_template.base_transforms = [prediction_recipe_transform]
    prediction_recipe_template.publish("transforms/Prediction.ipynb")


.. code:: ipython3

    prediction_recipe_transform = Transform()
    prediction_recipe_transform.templateId = prediction_recipe_template.id
    prediction_recipe_transform.name='prediction_val'
    prediction_recipe_transform.variables = {
        'inputDataset3': 'airline_grp_predict',

    #     'Sectors_Scheduled': 'Sectors_Scheduled',
        'Total_scheduled': 'Total_scheduled',
    #     'Sectors_Flown': 'Sectors_Flown',
        'Total_departed': 'Total_departed',

         'Cancelled_perc': 'Cancelled_perc',
        'Cancellations': 'Cancellations',
        'departure_delay_perc': 'departure_delay_perc',
        # 'Departures_Delayed': 'Departures_Delayed',
        'Departures_On_Time' : 'Departures_On_Time',
        'arrival_delay_perc': 'arrival_delay_perc',

        'Dep_Arr_delay': 'Dep_Arr_delay',
        'Arrivals_Delayed': 'Arrivals_Delayed',
    #     'Departing_Port': 'Departing_Port',
    #     'Arriving_Port': 'Arriving_Port',

    #     'Month_Num' : 'Month_Num',
    #     'Year'     : 'Year',
         'Btwness': 'Btwness',
        'closeness': 'closeness',
        'Deg_centrality': 'Deg_centrality',


        'outputDataset3': 'val_result',

        "modelName":"XGB_Shap_model"



    }



    prediction_recipe.add_transform(prediction_recipe_transform)

.. code:: ipython3

    prediction_recipe.run()

